# 多核程序设计与实践-书面报告



## 1. 选题介绍：DBSCAN聚类算法


DBSCAN(Density-Based Spatial Clustering of Applications with Noise)是一基于密度的聚类算法，能够把具有足够高密度的区域划分为簇，并可在噪声的空间数据库中发现任意形状的聚类，在许多领域（如天文学等）均被广泛应用。

下图展示了DBSCAN的示例分类结果，其分辨非线形聚类的能力是K-means等其他聚类算法所不能达到的：

<img src="./asset/DBSCAN-effect.svg" alt="DBSCAN-effect" style="zoom: 80%;" />



### 1.1 算法原理

在二维空间中，设数据集为$D$, 数据集中的任意点$p \in D$的$\epsilon$邻域为：以$p$为圆心，半径为$\epsilon$的圆。

根据数据点可分为以下三类：
1. 核心点：$p$的$\epsilon$邻域内点数超过$MinPts$，则$p$是核心点，且$p$的$\epsilon$邻域内的所有点都由$p$直接可达。
2. （密度）可达点：如果存在一条道路$p_1, ..., p_n$，其中$p_1 = p， p_n = q$，每一个点$p_{i}$到$p_{i + 1}$都是直接可达，则$q$是$p$的（密度）可达点。
3. 噪音点：不是核心点也不是可达点的数据点都属于噪音。

DBSCAN的基本思想是：若$p$可达$q$，则$p、q$属于同一个聚类。

<img src="./asset/DBSCAN-Illustration.svg" alt="DBSCAN-Illustration" style="zoom:150%;" />




上图中，红色点为核心点，黄色点为红色点的可达点，因此红色、黄色点同属于一个聚类。蓝色点不是核心点或可达点，属于噪音点。

基于上述算法原理，我们将DBSCAN的算法流程伪代码简化如下：

```python
function DBSCAN(D, eps, MinPts):
   for 点p in 数据集D:
      if 点p已经被访问过:
         continue
      将点p标注为访问过
      p_neighbors := 点p的eps邻域内所有点
      if 点p不是核心点:
         将点p标注为噪音点
      else:
         标注点p属于新集群c
         for 点q in p_neighbors:
            if 点q还没有被访问过:
                将点q标注为访问过
                q_neighbors := 点q的eps邻域内所有点
                if 点q是核心点:
                        p_neighbors := p_neighbors join q_neighbors
            if 点q不属于任何聚类:
                标注点q也属于集群c
```



### 1.2 复杂度分析

在DBSCAN中，时间复杂度主要取决于$\epsilon$邻域的查询次数。每个点$p$都要查询并遍历一遍$\epsilon$邻域，则最坏的时间复杂度是$O(|D|^2)$，空间复杂度为$O(|D|)$。



### 1.3 并行化意义

DBSCAN的并行化优化有两方面的重要意义：
1. **优化时间复杂度**：在实际使用场景中，$O(|D|^2)$的时间复杂度在大部分情况下都是不可承受的；通过并行化可以优化时间复杂度，增加DBSCAN的实用性。
2. **处理大数据集**：DBSCAN在聚类过程中需要将整个数据集加载到内存中并存储中间结果，无法处理较大的数据集；我们可以将数据集划分为大小合适的分块，并行化地对每个分块进行局部聚类，最后合并聚类得到最终聚类，从而处理大数据集。



## 2. 优化目标

分析可知，朴素实现的DBSCAN算法有以下局限性：
1. DBSCAN算法中需要对数据集中所有点进行$\epsilon$邻域搜索，涉及大量耗时的距离计算；
2. DBSCAN算法为串行算法，没有完全利用CPU或GPU的处理能力；
3. DBSCAN算法需要将整个数据集加载到内存中并存储中间结果，由于内存限制，无法处理较大的数据集。

我们的优化目标是追寻Michael Gowanlock等人自2017至2019年发表的三篇论文，结合CUDA和OpenMP两个并行计算框架，实现多核CPU和GPU共享存储的优化DBSCAN聚类算法，突破朴素DBSCAN算法的局限性。



### 2.1 Hybrid-DBSCAN

Michael Gowanlock等人在2017年发表的论文*Clustering Throughput Optimization on the GPU*、2018年发表的论文*A Hybrid Approach for Optimizing Parallel Clustering Throughput using the GPU*中提出了Hybrid-DBSCAN算法。

Hybrid-DBSCAN相比于朴素实现的DBSCAN，主要改进如下：
1. 使用网格状的索引(index)，以减少$\epsilon$邻域计算中耗时的距离计算;
2. 基于网格索引，使用CUDA并行化$D$中各点的$\epsilon$邻域计算，构建出各点的邻居表；
3. 设计批处理方案，增量式地调用CUDA计算数据集各点的$\epsilon$邻域，各批次之间使用OpenMP并行处理。批处理方案有以下优点：
    - 能处理超出显存限制的结果集（可达关系的集合）；
    - 使用多个CUDA stream分别处理各批次，并行化GPU计算和GPU-Host之间的数据传输（某个stream在进行计算的同时，另一个stream可以利用空闲的带宽进行数据传输）。



### 2.2 BPS-DBSCAN

Michael Gowanlock等人在2019年发表的论文*Hybrid CPU/GPU Clustering in Shared Memory on the Billion Point Scale*中提出了BPS-DBSCAN算法。

BPS-DBSCAN是基于Hybrid-DBSCAN的改进版。在Hybrid-DBSCAN的基础上，BPS-DBSCAN进行了以下更进一步的改进：
1. 提出了DenseBox算法，大幅减少$\epsilon$邻域计算中耗时的距离计算（DenseBox算法的具体细节在3.2.2小节给出）;
2. 对数据集进行分块处理，使用OpenMP并行化各数据块的局部聚类，再合并各局部聚类。
    - 此分块处理大大增加了可处理的数据集的量级，能处理超过GPU显存限制的数据集及中间结果，论文显示该方法可处理百万级别的数据集。



## 3. 实际优化内容

我们基本实现了论文所提出的Hybrid-DBSCAN和BPS-DBSCAN算法。囿于时间和设备限制，我们没有进行多GPU计算，或实现论文中提出的其他改进方案。

以下是我们所实现的优化内容：



### 3.1 Hybrid-DBSCAN

#### 3.1.1. 网格索引(index)

为了减少在查找$\epsilon$邻域时的距离计算，我们以$\epsilon$为边长，将数据空间划分为若干个网格，设每个网格的编号为`cellID`，数据点$p \in D$散落在各网格中，如下图所示。

<img src="./asset/index-cellID.svg" alt="index-cellID" style="zoom: 60%;" /><img src="./asset/index-gridID.svg" alt="index-gridID" style="zoom:60%;" />

可见，空网格没有存储价值，假设数据点$p$落在某个网格$c$中，则在$p$的$\epsilon$邻域的数据点必然只可能在与$c$相邻的9个网格中的非空网格中。因此，我们根据网格编号`cellID`的大小顺序，给所有非空网格一个新编号`gridID`。

为了计算邻域，我们还需要快速查询每个非空网格中的所有数据点。因此，我们按照数据点所在的非空网格的`gridID`的大小顺序，给每个数据点一个新的有序编号`orderedID`，使得在同一个非空网格中的所有数据点的`orderedID`是连续的一段。这样，我们就可以通过记录该非空网格中最小的`orderedID`和最大的`orderedID`来定位所有落在该网格中的数据点。


实现于`DBSCAN::constructIndex`。

#### 3.1.2. CUDA并行化$\epsilon$邻域计算并创建邻居表

由于数据点的邻域计算没有依赖关系，我们让每个线程计算一个数据点的$\epsilon$邻域。虽然使用CUDA并行化计算$\epsilon$邻域的好处是明显的，但也带来了数据传输的问题。如果把每个线程计算完的邻域都直接传输回host内存，带来的开销会很大：
- 由于不能提前知道每个数据点的邻域有多大，只能给每个线程分配$|D|$（邻域最多$|D|$个点）的空间，所有线程则需要$|D|^2$的数据传输；
- GPU数据传输到host的带宽被争抢。

因此，对于每个CUDA stream，我们在GPU的全局内存中申请一块缓冲区，把各数据点的可达关系resultSet存储在该缓冲区中。resultSet里存储的是若干个key-value对，表示`orderedID` = key的邻域中有`orderedID` = value的数据点。

kernel函数的伪代码如下：
```python
procedure kernel(D, index, eps): // D是分批后的数据集
    计算出thread的globalID
    通过globalID和数据所在批次，计算出数据点p的orderedID
    if orderedID >= |D|:
        return
    
    通过orderID找出数据所在的网格的gridID
    通过gridID找出该网格的相邻网格grids
    for each grid in grids:
        for each point q in grid:
            if dist(p, q) <= eps:
                # 注意：由于resultSet放在全局内存中，因此这一步是原子操作
                resultSet := resultSet join (p, q) 
```

由于各线程并发地读写resultSet，因此resultSet中key相同的键值对并不一定相邻。而构造邻居表的时候，我们需要的获取某个key的所有邻居，因此我们调用thrust库的sort-by-key函数，在GPU的全局内存中对resultSet进行排序，使得key相同的键值对相邻。

实现于`DBSCAN::constructGPUResultSet`。

#### 3.1.3. 批处理

如前所述，为了：
- 能处理超出显存限制的resultSet
- 使GPU计算和GPU-Host之间的数据传输同步进行

我们对数据集进行批处理，通过简单的将`orderedID`分段，创建$n_b$个批次，使用OpenMP并行地计算每个批次的数据集的resultSet。

我们在GPU上创建了3个CUDA stream，将某一批次的计算任务分配给其中一个stream。每个stream都有对应一块全局内存缓冲区以存放resultSet；当一个stream在进行计算的同时，另一个stream可以利用空闲的带宽进行数据传输。

#### 3.1.4. 创建邻居表

<img src="./asset/neighborTables.svg" alt="neighborTables" style="zoom:50%;" />

由于resultSet在全局内存的缓冲区中，我们在它被刷新以前（即下一批次到来以前）把它传输到host内存（使用thrust库的device_ptr来完成此操作），根据resultSet创建邻居表。由于resultSet已经是有序的，key相同的键值对必然相邻，所以数据点的邻居表只需要存储其对应key在resultSet中的起始下标和终止下标即可。

实现于`DBSCAN::constructNeighborTable`。

#### 3.1.5. 使用邻居表的修改版DBSCAN

我们已经提前计算得邻居表，因此我们需要修改DBSCAN使其使用邻居表。此步骤比较简单，不在此赘述。

实现于`DBSCAN::DBSCANwithNeighborTable`。

#### 3.1.6. Hybrid-DBSCAN算法伪代码

```
procedure Hybrid-DBSCAN(D, eps, MinPts):
    index := constructIndex(D, eps)
    初始化neighborTables
    按照orderedID将数据集D分为N个batch
    
    #pragma omp parallel for
    for each batch b:
        resultSet := constructGPUResultSet(b)
        sortByKey(resultSet) # 在显存上进行，使用thrust库
        neighborTables.join(constructNeighborTable(resultSet))
    
    DBSCANwithNeighborTable(neighborTables) # 修改版DBSCAN
```

### 3.2 BPS-DBSCAN

#### 3.2.1 数据分块(Partition)

当数据集相当巨大的时候，普通聚类算法不能保证内存和显存可以容纳完整的数据集。因此，我们实现了一个数据分块的功能。它可以把完整数据集分割成多块独立聚类，再在聚类结束之后把各个分块合并。具体方法为在数据集第一维上进行分割，并使得每一块包含的数据点大致相同，如下图所示：

![Partition](https://本地添加)

另外，如果内存显存允许，不同的数据块聚类过程可以并行执行，达到加速的效果。事实上，这种分块算法并不局限于DBSCAN聚类算法，它甚至可以推广到任意聚类算法上。数据的分块合并过程可以记为如下几个步骤。

1. 计算分割边界

    为了让每一块分块里的数据大致相同，我们首先需要在数据集第一维上以$\epsilon$为步长分割成许多个Cells，然后统计每一个Cell里面的数据点数。接着我们就能确认那些Cell之间为分割边界，使得每两个边界之间数据点的数量都大致为$\frac{数据总量}{分割块数}$。将分割边界记录下来。
2. 数据分块

    有了分割边界，我们就可以将数据分割。由于分割边界已知，分割不同数据块直接没有了依赖，所以这个过程我们可以利用openmp并行执行。分割同时，我们需要记录分割后的数据在子块里的下标和分割前数据在总块里的下标之间的映射，这样最后才能将子块合并。除此之外，我们还需要将边界附近Cells的点记录下来，称之为shadowCells。用于之后不同块之间聚类簇的合并。同样的，我们也需要一份shadowCells下标的映射表。
3. 子块独立聚类

    对每一个子块独立聚类就好了。这个过程也可以利用openmp并行执行。
4. 聚类结果合并

    当子块聚类完成后，我们需要将它们的聚类结果合并。对于不同子块聚类算法返回的聚类簇，我们只需要考虑上面提到的shadowCells里面的点就可以了。一个shadowCells的例子如下：
    ![shadowCells](https://本地添加)
    在这里面进行一次聚类，在聚类过程中判断是否出现了不同子块的点被聚类成同一类。若出现了这种情况，则标记这两类为同一类。最后，利用上面记录的下标映射表，我们将修正之后的聚类簇重写到一个完整的聚类簇上，实现完整的聚类。
    
    

#### 3.2.2 DenseBox算法

DenseBox算法被用来消除距离过近的点之间的距离计算过程，我们以$\frac{\epsilon}{2\sqrt{2}}$的距离划分网格，并记录每个点属于哪一个网格，如果一个网格中的点的数量超过minpts，就认为这是一个非空的网格，否则就将这些点记录下来，使用带有邻居表的DBSCAN算法对这些点单独聚类，最后合并所有的网格，合并的过程可以记为以下$5$个步骤:

1. 分离稀疏点

   首先遍历所有点，将每个点放入其对应的网格内，然后计算每个网格中存在的点的数目，如果一个网格中存在超过minpts个点，就认为这个网格非空，并且标记网格中的所有点属于同一个类，否则将这个网格中的所有点加入集合$L$。所有非空网格聚类的结果记为$C^{DBox}$
   
2. 对$L$集聚类

   对于集合$L$中的点，首先使用GPU加速建立邻居表，然后使用邻居表加速DBSCAN对$L$聚类，记聚类的结果为$C^{DBSCAN}$。

3. 合并相邻网格

   显然，在以$\frac{\epsilon}{2\sqrt{2}}$为间隔分割出的网格中，相邻两个网格中的点之间的距离一定小于$\epsilon$，如下图所示。
   
   $图$
   
   所以我们可以遍历一遍非空的网格，将所有两两相临的网格中的点合并为一类。
   
3. 合并不相邻网格


4. 合并DenseBox结果和DBSCAN结果

#### 3.1.6. BPS-DBSCAN算法伪代码

```
procedure BPS-DBSCAN(D, eps, MinPts, N_p):
    初始化总聚类簇 C
    初始化N_p个子聚类簇 Cs
    计算边界Bs
    Ds, shadowCells := Partition(D, Bs, N_p)
    #pragma omp parallel for
    for D' in Ds:
        if denseBox:
            subC = DenseBox(D', eps, MinPts)
        else:
            subC = Hybrid-DBSCAN(D', eps, MinPts)
        
        #pragma omp critical    //修改外部变量需要避免访问冲突
        Cs[i] = subC
    
    C = merge(Cs, shadowCells)
```

## 4. 具体实验方案

代码结构（参见Code文件夹）如下：
```bash
.
├── .clang-format # 代码格式化
├── data # 数据集
│   ├── input # 测试数据集
│   └── output # 输出结果集
├── .gitignore
├── include # 三种DBSCAN方法及辅助函数的声明
│   ├── BPSDBSCAN.hpp
│   ├── Dataset.hpp
│   ├── DBSCAN.hpp
│   ├── DenseBox.hpp
│   ├── global.hpp
│   ├── HybridDBSCAN.hpp
│   ├── OriginalDBSCAN.hpp
│   ├── Shadower.cuh
│   └── utils.hpp
├── lib # 三种DBSCAN方法及辅助函数的具体实现
│   ├── BPSDBSCAN.cpp
│   ├── Dataset.cpp
│   ├── DBSCAN.cu
│   ├── DenseBox.cpp
│   ├── HybridDBSCAN.cu
│   ├── OriginalDBSCAN.cpp
│   ├── Shadower.cu
│   └── utils.cpp
├── makefile 
└── src # 主函数
    └── main.cpp
```

运行方式：
```bash
make clean
make # 生成target文件夹和obj文件夹
make all # 生成所有obj文件和可执行文件
./target/multicoreDBSCAN <Dataset file>, <Epsilon>, <MinPts>, <Partitions>, <BlockSize>
```

### 4.1. 辅助函数

#### 4.1.1. 数据集的载入及处理

我们将数据集封装成类`Dataset`, 封装了数据集的一系列操作：
```c++
class Dataset {
public:
    // 将txt数据集转换成二进制数据集，用于压缩大数据集
    int genDatasetFromTxt(const std::string fromTxt, const std::string toBin);
    // 数据集的读入
    void loadDataset(const std::string file);
    // 数据集的显示
    void showDataset(int head = 100);

    // Get Funtions
    const DataPointsType& getDataset();
    
private:
    // ... 省略
};
```

声明于`include/dataset.hpp`，实现于`lib/dataset.cpp`。

#### 4.1.2. 父类DBSCAN

由于我们将实现三种不同的DBSCAN算法：
- 朴素实现的DBSCAN（Original-DBSCAN）
- Hybrid-DBSCAN
- BPS-DBSCAN

我们将以上三种算法均封装成类，继承自父类DBSCAN。父类DBSCAN起到规范接口、让三种算法共用父类方法的作用。

由于构造索引index、构造结果集resultSet、构造邻居表、实现使用邻居表的DBSCAN是Hybrid-DBSCAN和BPS-DBSCAN都会调用的方法，故将其实现为父类DBSCAN的方法（参数较长，在此报告中省略，详情请见代码）。

```c++
class DBSCAN {
public:
    /** 定义纯虚函数，作为接口 **/
    // 算法运行
    virtual void run() = 0;
    // 输出结果到指定文件
    virtual void print(const string &outFile) = 0;
    
protected:
    // 遍历数据集，将数据空间划分成以eps为边长的网格
    void calcCells(/** 参数较长，省略 **/);
    // 创建网格索引
    void constructIndex(/** 参数较长，省略 **/);
    // 调用核函数，计算resultSet
    float constructGPUResultSet(/** 参数较长，省略 **/);
    // 根据resultSet建立邻居表
    void constructNeighborTable(/** 参数较长，省略 **/);
    // 修改后的使用邻居表的DBSCAN
    void DBSCANwithNeighborTable(/** 参数较长，省略 **/);
};
```

其中，关键的操作是构建resultSet和根据resultSet建立邻居表，在Hybrid-DBSCAN中由于批处理的需要实现了这两个方法合为一体并加上批处理的方法，因此在此不再赘述实现细节。

声明于`include/DBSCAN.hpp`，实现于`lib/DBSCAN.cpp`。


### 4.2. Original-DBSCAN

作为基线，我们实现了朴素实现DBSCAN算法。

关键代码如下：
```c++
void OriginalDBSCAN::run() {
    // ... 省略
    int clusterID = 0;
    for (uint i = 0; i < dataSize; i++) {
        // 如果数据点已经被访问过，说明已经被聚类或者是噪音点
        if (clusterIDs[i] == DBSCAN::UNVISITED) {
            // 暴力求邻域（遍历所有数据点）
            auto neighbors = getNeighbors(i);
            if ((neighbors.size() + 1) < minpts) {
                // 噪音点
                clusterIDs[i] = DBSCAN::NOISE;
            } else {
                // 核心点
                clusterIDs[i] = ++clusterID;
                // 遍历可达点
                for (uint j = 0; j < neighbors.size(); j++) {
                    uint p = neighbors[j];
                    if (clusterIDs[p] == DBSCAN::UNVISITED) {
                        auto pNeighbors = getNeighbors(p);
                        if ((pNeighbors.size() + 1) >= minpts) {
                            // p也是核心点，则可达点增多
                            neighbors.insert(neighbors.end(), pNeighbors.begin(), pNeighbors.end());
                        }
                    }
                    // 如果点p尚未被聚类，已知p可达，故同属一个聚类
                    if (clusterIDs[p] == DBSCAN::UNVISITED || clusterIDs[p] == DBSCAN::NOISE) {
                        clusterIDs[p] = clusterID;
                    }
                }
            }
        }
    }
    // ... 省略
```

声明于`include/OriginalDBSCAN.hpp`，实现于`lib/OriginalDBSCAN.cpp`。

### 4.3. Hybrid-DBSCAN

构造索引index、构造结果集resultSet、构造邻居表、实现使用邻居表的DBSCAN等操作已抽出并实现于父类DBSCAN。

为了实现Hybrid-DBSCAN的批处理，我们将构造结果集resultSet、构造邻居表这两个方法合并为新方法`constructResultSetAndNeighborTable`，这也是Hybrid中最关键的操作（为了实现方便，同样将其放在父类中，代价是调用时参数较多）。

```c++
void DBSCAN::constructResultSetAndNeighborTable(
    const uint &NCHUNKS, // 批处理批数
    const array<uint, 2> &nCells, // 数据空间中各维度的网格数
    const uint &gridSize, // 非空网格数
    const vector<uint> &ordered2GridID, // orderedID对应的数据点所在的非空网格ID的转换表
    const vector<uint> &ordered2DataID, // orderedID到DataID的转换表
    const vector<uint> &grid2CellID, // 非空网格ID到cellID的转换表
    const vector<Grid> &index, // 索引
    const uint &blockSize,
    uint neighborsCnts[GPU_STREAMS],
    uint *orderedIDKeys[GPU_STREAMS],
    uint *orderedIDValues[GPU_STREAMS],
    vector<uint *> &valuePtrs,
    vector<NeighborTable> &neighborTables) {
    uint chunkSize = dataSize / NCHUNKS;

    /** 
    将数据集dataPoints、ordered2DataID、
    orderedID2GridID、grid2CellID、index
    从内存拷贝到GPU的全局存储
    **/
    float *d_dataPoints;
    uint *d_ordered2DataID, *d_ordered2GridID, *d_grid2CellID;
    Grid *d_index;
    // 拷贝过程省略...
    /**
    为每个CUDA Stream都申请一块全局内存空间，用于缓存resultSet。
    resultSet由若干对键值对组成，key和value均是数据点的orderedID
    **/
    uint *d_orderedIDKeys[GPU_STREAMS];
    uint *d_orderedIDValues[GPU_STREAMS];
    for (uint i = 0; i < GPU_STREAMS; i++) {
        CHECK(cudaMalloc((void **)&d_orderedIDKeys[i], sizeof(uint) * GPU_BUFFER_SIZE));
        CHECK(cudaMalloc((void **)&d_orderedIDValues[i], sizeof(uint) * GPU_BUFFER_SIZE));
    }
    /**
    为每一个Stream创建一个计数器cnt，
    用于记录该Stream存储的resultSet中的键值对数量
    **/
    uint *d_cnt;
    CHECK(cudaMalloc((void **)&d_cnt, sizeof(uint) * GPU_STREAMS));
    // 创建CUDA Stream
    cudaStream_t stream[GPU_STREAMS];
    for (uint i = 0; i < GPU_STREAMS; i++) {
        CHECK(cudaStreamCreateWithFlags(&stream[i], cudaStreamNonBlocking));
    }

    /*** 批处理并行计算resultSet（CPU开启GPU_STREAMS个线程） ***/
#pragma omp parallel for schedule(dynamic, 1) num_threads(GPU_STREAMS)
    for (uint i = 0; i < NCHUNKS; i++) {
        // 将每个批次分配给对应的stream
        uint chunkID = i;
        int streamID = omp_get_thread_num();
        // 清空当前子resultSet计数
        neighborsCnts[streamID] = 0;
        CHECK(cudaMemcpyAsync(&d_cnt[streamID], &neighborsCnts[streamID], sizeof(uint), cudaMemcpyHostToDevice, stream[streamID]));
        // 使用一维模型，调用核函数
        dim3 dimGrid(ceil((double)chunkSize / (double)blockSize));
        dim3 dimBlock(blockSize);
        gpuCalcGlobal<<<dimGrid, dimBlock, 0, stream[streamID]>>>(chunkID, NCHUNKS, chunkSize, dataSize, nCells[1], gridSize, epsilonPow, d_dataPoints, d_ordered2DataID, d_ordered2GridID, d_grid2CellID, d_index, &d_cnt[chunkID], d_orderedIDKeys[streamID], d_orderedIDValues[streamID]);
        // 将计数结果从显存拷贝回内存
        CHECK(cudaMemcpyAsync(&neighborsCnts[streamID], &d_cnt[streamID], sizeof(uint), cudaMemcpyDeviceToHost, stream[streamID]));
        /** 
        利用thrust库，在显存中对resultSet的键值对按照key进行排序，
        使得key相同的键值对相邻
        **/
        device_ptr<uint> d_keyPtr(d_orderedIDKeys[streamID]);
        device_ptr<uint> d_valuePtr(d_orderedIDValues[streamID]);
        try {
            sort_by_key(thrust::cuda::par.on(stream[streamID]), d_keyPtr, d_keyPtr + neighborsCnts[streamID], d_valuePtr);
        } catch (std::bad_alloc &e) {
            fprintf(stderr, "Error: bad alloc when sorting.\n");
            exit(-1);
        }
        // 排序完成后，将resultSet拷贝回内存
        CHECK(cudaMemcpyAsync(raw_pointer_cast(orderedIDKeys[streamID]), raw_pointer_cast(d_keyPtr), sizeof(uint) * neighborsCnts[streamID], cudaMemcpyDeviceToHost, stream[streamID]));
        CHECK(cudaMemcpyAsync(raw_pointer_cast(orderedIDValues[streamID]), raw_pointer_cast(d_valuePtr), sizeof(uint) * neighborsCnts[streamID], cudaMemcpyDeviceToHost, stream[streamID]));
        // stream同步
        CHECK(cudaStreamSynchronize(stream[streamID]));
        // 利用刚计算得的resultSet构造邻居表
        valuePtrs[chunkID] = new uint[neighborsCnts[streamID]];
        constructNeighborTable(orderedIDKeys[streamID], orderedIDValues[streamID], neighborsCnts[streamID], valuePtrs[chunkID], neighborTables);
    }

    // 释放显存空间 （省略）
    // 销毁 CUDA Stream
    for (int i = 0; i < GPU_STREAMS; i++) {
        CHECK(cudaStreamDestroy(stream[i]));
    }
}
```

其中，核函数`gpuCalcGlobal`的作用是让每个线程计算一个点的邻域。
在添加批处理后，每一个线程与其处理的数据点的对应关系如下：
![加图](https://)
核函数的实现中需要注意的是，由于resultSet存储在全局内存中，为了避免数据竞争，添加键值对到resultSet时必须采用原子操作。

```c++
__global__ void gpuCalcGlobal(
    const uint chunkID, // 批处理的批次
    const uint NCHUNKS,  // 批处理批数
    const uint dataSize, // 数据集大小
    const uint nCells1, // 用于计算cellID
    const uint gridSize, // 非空网格数
    const float epsilonPow, // eps^2
    const float *d_dataPoints, // 数据集
    const uint *d_ordered2DataID, // orderedID到dataID的转换表
    const uint *d_ordered2GridID, // orderedID所在的非空网格ID的查询表
    const uint *d_grid2CellID, // 非空网格对应的cellID的查询表
    const Grid *d_index, // 索引
    uint *d_cnt, // 计数
    uint *d_orderedIDKey, // resultSet键
    uint *d_orderedIDValue // resultSet值
) {
    uint globalID = blockIdx.x * blockDim.x + threadIdx.x;
    if (globalID >= chunkSize) return;

    // 计算orderedID，需要看分块
    uint orderedID = globalID * NCHUNKS + chunkID;
    // 获取orderedID对应的数据点所在的非空网格编号
    uint gridID = d_ordered2GridID[orderedID];
    uint cellID = d_grid2CellID[gridID];
    uint newCellID = 0;
    // 查找相邻的非空网格
    for (int i = -1; i <= 1; i++) {
        for (int j = -1; j <= 1; j++) {
            if (i == 0 && j == 0) {
                gridID = d_ordered2GridID[orderedID];
            } else {
                newCellID = cellID + (i * nCells1 + j);
                // 由于网格cell多，非空网格少，
                // 所以建立cellID到gridID的查询表将非常稀疏（浪费空间），
                // 因此每次直接二分查找
                gridID = findGridByCellID(newCellID, gridSize, d_grid2CellID);
            }
            // 如果该非空网格存在
            if (gridID < gridSize) {
                // 遍历网格中每一个数据点
                for (uint k = d_index[gridID].orderedID_min; k <= d_index[gridID].orderedID_max; k++) {
                    // 排除自己
                    if (orderedID == k) {
                        continue;
                    }
                    // 如果距离小于eps，是可达点
                    if (gpuInEpsilon(orderedID, k, epsilonPow, dataSize, d_dataPoints, d_ordered2DataID)) {
                        // 原子操作，防止数据竞争
                        uint idx = atomicAdd(d_cnt, int(1));
                        d_orderedIDKey[idx] = orderedID;
                        d_orderedIDValue[idx] = k;
                    }
                }
            }
        }
    }
}
```


### 4.4. BPS-DBSCAN

DenseBox算法封装成类`DenseBox`， ...



### 4.5. 实验过程中遇到的问题及解决方案

原本项目的构建是通过CMake管理的，`lib`文件夹将被编译成动态库，连接到`main.cpp`编译而成的可执行文件。但奇怪的是，这样做虽然`.cu`文件能调用OpenMP，但设置线程数失效，每次OpenMP并行都只有一个线程。

猜测原因是因为要将`lib`编译成库，在CMake中设置了CUDA单独编译，CMake指令`set(CMAKE_C_FLAGS "${CMAKE_C_FLAGS} ${OpenMP_C_FLAGS}")`实际上没有把OpenMP库的链接指令传递给`nvcc`（只传递给了`gcc`)。在经过多次解决尝试未果后，放弃CMake，直接采用make，编写了现在代码中的makefile（直接将`-fopenmp`作为`nvcc`的FLAG），成功解决问题。

另外，由于此项目是小组合作共同开发，在汇总阶段我们发现在自己本地能正确运行的代码在对方的机器可能无法运行。其中有：
- 出现thrust库的报错指令。解决方案：发现是GPU架构的不同导致的问题，需要正确设置编译FLAG，如`-arch=compute_60 -code=sm_60`只适用于Pascal架构、CUDA 8以上版本。（参考链接：https://arnon.dk/matching-sm-architectures-arch-and-gencode-for-various-nvidia-cards/）
- 出现段错误或访存错误。解决方案：该问题往往是因为处理大数据集时内存或显存不足引起的，不同的机器的内存或显存大小不同，适当增加分块数或批数则能成功解决。


## 5. 实验结果

我们试验了6个数据集，大小分别为2500、8000、10000、20000、100000和2400000。下面是不同聚类算法在2500个点的聚类结果，验证了正确性。

![Original DBSCAN](https://本地添加)

![Hybrid DBSCAN](https://本地添加)

![BPS DBSCAN](https://本地添加)

## 5.1. blockSize 对比实验

### 5.1.1. 实验数据及可视化

我们测试了epsilon = 0.1, MinPts = 3, NCHUNK = 4的情况下不同blockSize的聚类时间

| blockSize= 4 | Original DBSCAN  | Hybrid DBSCAN   | BPS DBSCAN      |
| ------------ | ---------------- | --------------- | --------------- |
| 2500         | 298.364ms        | 221.976ms       | 293.453ms       |
| 8000         | 2907.74ms        | 299.954ms       | 323.34ms        |
| 10000        | 4578.22ms        | 373.924ms       | 463.37ms        |
| 20000        | 18077.9ms        | 507.149ms       | 1425.45ms       |
| 100000       | 449588ms         | 893.066ms       | 2362.1ms        |
| 2400000      | too long to wait | memory overflow | memory overflow |

| blockSize= 8 | Original DBSCAN  | Hybrid DBSCAN   | BPS DBSCAN      |
| ------------ | ---------------- | --------------- | --------------- |
| 2500         | 298.364ms        | 303.293ms       | 342.635ms       |
| 8000         | 2907.74ms        | 400.46ms        | 325.43ms        |
| 10000        | 4578.22ms        | 288.857ms       | 453.34ms        |
| 20000        | 18077.9ms        | 422.2ms         | 1256.45ms       |
| 100000       | 449588ms         | 1143.84ms       | 2546.21ms       |
| 2400000      | too long to wait | memory overflow | memory overflow |

| blockSize= 16 | Original DBSCAN  | Hybrid DBSCAN   | BPS DBSCAN      |
| ------------- | ---------------- | --------------- | --------------- |
| 2500          | 298.364ms        | 269.918ms       | 323.764ms       |
| 8000          | 2907.74ms        | 311.902ms       | 347.345ms       |
| 10000         | 4578.22ms        | 420.948ms       | 623.642ms       |
| 20000         | 18077.9ms        | 461.208ms       | 1324.46ms       |
| 100000        | 449588ms         | 1186.39ms       | 2643.34ms       |
| 2400000       | too long to wait | memory overflow | memory overflow |

| blockSize= 32 | Original DBSCAN  | Hybrid DBSCAN   | BPS DBSCAN      |
| ------------- | ---------------- | --------------- | --------------- |
| 2500          | 298.364ms        | 222.95ms        | 353.764ms       |
| 8000          | 2907.74ms        | 293.811ms       | 356.345ms       |
| 10000         | 4578.22ms        | 320.445ms       | 624.642ms       |
| 20000         | 18077.9ms        | 507.582ms       | 1573.6ms        |
| 100000        | 449588ms         | 970.497ms       | 2574.4ms        |
| 2400000       | too long to wait | memory overflow | memory overflow |

将上述实验结果可视化，得：
![待会加图](https://)

### 5.1.2. 结果分析

可见，随着blockSize的从4倍增到32，基线（Original DBSCAN）不受影响，Hybrid DBSCAN呈现 .... ，BPS DBSCAN呈现 ....

... 各种现象的原因是 ... （不知道咋写了）


## 5.2. NCHUNKS 对比实验

变量NCHUNKS在不同的算法中表示不同的含义：
- 在Hybrid-DBSCAN中，NCHUNKS表示批处理的分批数；
- 在BPS-DBSCAN中，NCHUNKS表示数据集的分块数。

### 5.2.1. 实验数据及可视化

我们测试了epsilon = 0.1, MinPts = 3, blockSize = 16 的情况下不同blockSize的聚类时间：

| NCHUNK = 3 | Original DBSCAN  | Hybrid DBSCAN   | BPS DBSCAN      |
| ---------- | ---------------- | --------------- | --------------- |
| 2500       | 298.364ms        | 410.389ms       | 402.754ms       |
| 8000       | 2907.74ms        | 367.757ms       | 382.765ms       |
| 10000      | 4578.22ms        | 444.726ms       | 657.465ms       |
| 20000      | 18077.9ms        | 623.967ms       | 1224.43ms       |
| 100000     | 449588ms         | 892.392ms       | 2024.2ms        |
| 2400000    | too long to wait | memory overflow | memory overflow |

| NCHUNK = 4 | Original DBSCAN  | Hybrid DBSCAN   | BPS DBSCAN      |
| ---------- | ---------------- | --------------- | --------------- |
| 2500       | 298.364ms        | 269.918ms       | 323.764ms       |
| 8000       | 2907.74ms        | 311.902ms       | 347.345ms       |
| 10000      | 4578.22ms        | 420.948ms       | 623.642ms       |
| 20000      | 18077.9ms        | 461.208ms       | 1324.46ms       |
| 100000     | 449588ms         | 1186.39ms       | 2643.34ms       |
| 2400000    | too long to wait | memory overflow | memory overflow |

| NCHUNK = 5 | Original DBSCAN  | Hybrid DBSCAN   | BPS DBSCAN      |
| ---------- | ---------------- | --------------- | --------------- |
| 2500       | 298.364ms        | 275.717ms       | 343.362ms       |
| 8000       | 2907.74ms        | 342.37ms        | 357.635ms       |
| 10000      | 4578.22ms        | 435.416ms       | 642.753ms       |
| 20000      | 18077.9ms        | 493.21ms        | 1373.75ms       |
| 100000     | 449588ms         | 972.451ms       | 2445.2ms        |
| 2400000    | too long to wait | memory overflow | memory overflow |

经过测试，只有当NCHUNK=25的时候才能成功运行2400000的数据集：

| NCHUNK = 25 | Original DBSCAN  | Hybrid DBSCAN | BPS DBSCAN |
| ----------- | ---------------- | ------------- | ---------- |
| 2500        | 298.364ms        | 319.852ms     | 352.623ms  |
| 8000        | 2907.74ms        | 402.278ms     | 393.54ms   |
| 10000       | 4578.22ms        | 525.369ms     | 453.32ms   |
| 20000       | 18077.9ms        | 455.955ms     | 1125.25ms  |
| 100000      | 449588ms         | 844.597ms     | 2162.2ms   |
| 2400000     | too long to wait | 5832.29ms     | 6435.34ms  |

将上述实验结果可视化，得：
![待会加图](https://)

#### 5.2.2. 结果分析

### 5.3. 实验结果总结



## 附录

